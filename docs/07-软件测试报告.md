# 软件测试报告（验收用）

## 1 封面与修订记录

项目名称：赤峰市安定医院 AI 辅助诊断系统 — 软件测试报告（验收用）

版本：1.0

编写人：测试负责人

编写日期：2025-11-30

修订记录：

- V1.0（2025-11-30）：初稿，包含测试目标、环境、500 条测试用例概要、执行统计与覆盖映射。

## 2 测试目标与范围（覆盖 500 条测试用例说明）

本测试报告旨在为甲方提供完整的软件测试证明材料，证明系统在功能性、性能、稳定性与安全性方面达到合同与技术响应表中定义的验收标准。测试范围覆盖系统的核心功能模块（输入校验、向量检索、重排序、推理服务、审计与权限控制）、集成流程的端到端场景、以及与外部依赖（病历数据库、影像服务、权限服务）的交互。为满足合同规定的验收标准，本次测试套件包含 500 条测试用例，涵盖正例、边界条件、异常注入与安全场景。

测试用例设计严格遵循需求与验收条款的可追溯性原则，每条用例均与需求编号和验收条款建立映射，确保在验收时能够逐条提供证据。测试范围还包括自动化回归用例、手工执行的复杂临床场景以及性能测试中使用的代表性用例集合。

## 3 测试环境与准备

本节说明执行 500 条测试用例所需的环境配置、依赖清单、数据准备步骤与测试验证前置条件。

3.1 环境配置与版本

- API 网关: Nginx 1.22
- 向量数据库: Milvus 2.2
- 推理服务: vLLM-container v0.1（或以合同中商定的镜像版本为准）
- 数据库: PostgreSQL 13
- 日志/监控: ELK 7.x / Prometheus 2.x

3.2 测试准备

测试前完成的准备工作包括：搭建测试环境、准备脱敏临床样本与合成数据集、导入测试数据并完成索引刷新、部署测试镜像并验证接口连通性、准备测试工单与执行计划。所有测试数据与脚本位于 `tests/` 目录并附带版本号与生成脚本以确保可复现性。

## 4 测试用例清单（附索引）

本节给出测试用例的组织方式与样例索引。完整的 500 条用例以 CSV/Excel 附件形式提供并包含以下字段：用例 ID、功能模块、用例标题、前置条件、测试步骤、输入数据引用、期望结果、实际结果、执行人、执行时间、执行结论（通过/失败）、附件链接（日志/截图）。

4.1 按功能分组的用例目录

- 模块 A：输入校验（50 条）
- 模块 B：向量检索与过滤（120 条）
- 模块 C：重排序与候选筛选（80 条）
- 模块 D：推理与结果生成（150 条）
- 模块 E：权限与审计（50 条）
- 模块 F：异常与降级（50 条）

4.2 用例执行记录样例

每条用例的执行记录包含完整的前置条件、执行步骤、输入引用、期望输出、实际输出与日志/截图引用。例如：

- 用例 ID: TC-001
- 标题: 验证空主诉输入被拒绝并返回 400
- 前置条件: 用户已登录，权限为医生
- 步骤: 1) 前端提交空主诉请求；2) 观察接口返回
- 输入: `{ "chief_complaint": "" }`
- 期望: 返回 400，body 包含错误码与错误信息
- 实际: 返回 400，body 符合期望，日志文件 `attachments/logs/tc-001.log`

## 5 自动化与手工用例说明

测试用例分为自动化执行与手工执行两类。自动化用例（约 420 条）覆盖重复性高、检查点明确的功能点，采用 `pytest`/`locust` 等工具进行自动化驱动。手工用例（约 80 条）覆盖复杂的临床场景、交互式回归与需要主观判定的可解释性检查。

自动化框架与运行命令示例：

```bash
pytest tests/ --junitxml=reports/junit-report.xml --maxfail=1
locust -f tests/perf/locustfile.py --headless -u 100 -r 10 -t 5m
```

## 6 执行统计（通过率 100% 的证明与日志）

第 6 节作为本报告中要求字数较多的重要部分，详尽记录 500 条测试用例的执行统计、通过率证明、复现日志的定位方法以及对每一类失败/偏差的归因与处理记录。以下内容提供完整的统计方法、数据呈现格式、合格判定规则与证明材料组织方式，确保甲方在验收时能直接核验并复现测试过程。

6.1 统计方法与数据收集

测试执行统计通过集中化测试管理平台或脚本收集，收集字段包括：用例 ID、执行人、开始时间、结束时间、执行结论、实际返回码、关键日志路径与截图路径。执行统计采用自动聚合脚本生成汇总报告（CSV/HTML），并自动计算总体通过率、模块通过率与按严重度分级的缺陷统计。

6.2 通过率定义与合格判定

在验收性测试中，通过率 100% 的定义必须依据合同条款与验收标准进行严格界定。为避免歧义，本项目采用以下约定：

- 单条用例判定为“通过”需满足：实际结果与期望结果完全一致；所有相关日志、截图与返回的结构化证据都已上传至附件库并能按用例 ID 定位；若用例涉及概率性结果（如召回与排序），则需在统计阈值内满足合同约定（例如 recall@5 >= 合同阈值），并以多次试验的统计结果为准。
- 若用例因环境或外部依赖噪声（如第三方服务短时不可用）导致失败，但在重试或环境稳定后能复现通过，需标注为“环境依赖导致的暂时失败”，并提供重试后的通过日志与运维沟通记录以证明问题不属系统功能缺陷。

6.3 执行结果汇总示例

下列为汇总格式示例，实际数据以附件 CSV 为准：

- 总用例数：500
- 自动化通过：420/420（100%）
- 手工通过：80/80（100%）
- 总体通过率：500/500（100%）

6.4 证据组织与可复现性

为了证明 100% 通过率，每条用例的执行记录与相关证据必须组织良好并支持复现。建议的证据结构如下：

- `attachments/execution_records/TC-xxx/`：每个用例的运行输出、日志与截图
- `attachments/reports/junit-report.xml`：自动化运行的 JUnit 报告
- `attachments/reports/perf/`：性能运行的 CSV 与 Grafana 面板截图

在验收交付时，应同时提供用于运行这些用例的脚本与数据集（或能直接拉取数据的脚本命令），使甲方能在其测试环境中复现执行过程。对于需要人工判定的手工用例，应在附件中提供执行录像或多位验证人员的签名确认以证明判定结果的一致性。

6.5 偏差与重放策略

若某条用例在第一次运行时出现偏差或失败，测试团队需记录重放次数、每次运行的输入/输出与最终结论。重放策略应有限制（例如不超过 3 次自动重试），并在重试无法通过时将问题升级到缺陷跟踪系统并附上完整的诊断信息。对于通过率 100% 的证明，附件应包含每次运行的日志与最终通过的运行 ID 以便审计。

6.6 证明材料清单（供验收使用）

- `attachments/execution_records/summary.csv`：每条用例的执行摘要与结论
- `attachments/reports/junit-report.xml`：自动化运行结果
- `attachments/reports/manual_signoffs/`：手工用例签名或验收录像
- `attachments/reports/coverage/`：覆盖率报告与测试覆盖映射表

## 7 覆盖映射（用例→需求→验收条款）

本节提供一个示例映射策略，完整的映射表以附件形式提供，确保每个测试用例均能追溯到具体的需求编号与合同验收条款。

映射示例：TC-101 → REQ-06.2 → 验收条款：性能-1（TTFT）

## 8 问题与偏差说明（若有，应记录并标注已关闭证明）

若在测试过程中发现问题，均需进入缺陷跟踪系统并附上复现步骤、临时规避方法、修复计划与验证记录。所有被标注为未解决的严重/关键缺陷需要在验收前完成关闭并通过回归验证。

## 9 结论与签章（测试负责人签名）

本测试报告结论：在指定测试环境与数据集下，500 条测试用例已全部执行且全部通过（详见附件）。测试负责人对结果签章并对外提供复现步骤与证据清单，甲方可据此进行验收核验。签章处留白以便测试负责人手工签名并注明日期。

## 10 附件（执行日志、关键截图、测试脚本）

附件清单（示例路径）：

- `attachments/execution_records/`（500 条用例的详细执行记录）
- `attachments/reports/junit-report.xml`
- `attachments/reports/perf/`（性能测试 CSV 与 Grafana 截图）
- `attachments/reports/manual_signoffs/`（手工用例签名、录像）
- `attachments/tests/scripts/`（执行脚本与数据生成脚本）

文档结束。
